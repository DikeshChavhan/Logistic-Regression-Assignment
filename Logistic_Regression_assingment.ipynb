{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Logistic Regression | Assignment**"
      ],
      "metadata": {
        "id": "TTNBd0hDMx0i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is Logistic Regression, and how does it differ from Linear Regression?\n",
        "\n",
        "\n",
        " Answer: Logistic Regression is a supervised machine learning algorithm used for classification tasks, where\n",
        " the output variable is categorical (such as Yes/No or 0/1). It predicts the probability that an instance belongs\n",
        " to a particular class using the logistic (sigmoid) function. The model takes a linear combination of the input\n",
        " features and applies the sigmoid function to map the result between 0 and 1.\n",
        " The mathematical representation of Logistic Regression is: P(Y=1|X) = 1 / (1 + e^-(b0 + b1x1 + b2x2 + ... +\n",
        " bnxn))\n",
        " Key Points: - Logistic Regression is used when the dependent variable is categorical, while Linear Regression\n",
        " is used when it is continuous. - Linear Regression predicts a numeric value, but Logistic Regression predicts\n",
        " a probability value that is later converted to a class (0 or 1). - Logistic Regression uses log loss (cross\n",
        "entropy) as its cost function, whereas Linear Regression uses mean squared error (MSE). - The output of\n",
        " Linear Regression is unbounded, while Logistic Regression restricts the output between 0 and 1 using the\n",
        " sigmoid function.\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "lRLiTF7f884q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: Explain the role of the Sigmoid function in Logistic Regression.\n",
        "\n",
        "\n",
        " Answer: The Sigmoid function is the core of Logistic Regression, as it converts the output of a linear\n",
        " equation into a probability between 0 and 1. It ensures that predictions are interpretable as probabilities,\n",
        " which helps in classification.\n",
        " The formula for the sigmoid function is: σ(z) = 1 / (1 + e^-z), where z = b0 + b1x1 + b2x2 + ... + bnxn\n",
        " Role of the Sigmoid Function: - Converts any real-valued number into a probability value between 0 and 1.\n",
        "When z > 0, the output of sigmoid approaches 1, and when z < 0, it approaches 0. - Acts as a threshold\n",
        " function, helping the model classify data into classes. - Provides a smooth and differentiable output, which\n",
        " makes it suitable for gradient-based optimization methods"
      ],
      "metadata": {
        "id": "NQr4jKWJ9Gse"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Question 3: What is Regularization in Logistic Regression and why is it needed?\n",
        "\n",
        "\n",
        " Answer: Regularization in logistic regression is a technique used to reduce overfitting by adding a penalty\n",
        " term to the loss function. Overfitting occurs when a model learns unnecessary details from training data,\n",
        " reducing its performance on unseen data. Regularization helps keep the model simple and general.\n",
        " The regularized cost function is: J(θ) = -(1/m) Σ [y_i log(hθ(x_i)) + (1 - y_i) log(1 - hθ(x_i))] + λ R(θ), where R(θ) is\n",
        " the regularization term and λ is the regularization strength.\n",
        " 1\n",
        "Types of Regularization: - L1 Regularization (Lasso): Adds the absolute value of coefficients |wi| as a\n",
        " penalty, can shrink some coefficients to zero. - L2 Regularization (Ridge): Adds the square of coefficients\n",
        " wi^2 as a penalty, prevents large weights but keeps all features.\n",
        " Importance: - Controls the complexity of the model. - Prevents overfitting and improves generalization.\n",
        "Makes the model more stable and interpretable."
      ],
      "metadata": {
        "id": "OLNB7qPu9IDV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What are some common evaluation metrics for classification models, and why are they\n",
        " important?\n",
        "\n",
        "\n",
        " Answer: In classification problems, it is essential to evaluate a model using various metrics to understand its\n",
        " performance. Common evaluation metrics include: - Accuracy: Measures the ratio of correct predictions to\n",
        " total predictions. Accuracy = (TP + TN) / (TP + TN + FP + FN) - Precision: Indicates how many of the predicted\n",
        " positive cases were actually positive. Precision = TP / (TP + FP) - Recall (Sensitivity): Measures how well the\n",
        " model captures actual positive cases. Recall = TP / (TP + FN) - F1-Score: Harmonic mean of precision and\n",
        " recall. F1 = 2 * (Precision * Recall) / (Precision + Recall) - ROC-AUC Score: Evaluates how well the model\n",
        " distinguishes between classes.\n",
        " These metrics are important because relying on accuracy alone can be misleading, especially in imbalanced\n",
        " datasets. Precision, recall, and F1-score provide a deeper understanding of the model’s performance and\n",
        " help select the optimal decision threshold."
      ],
      "metadata": {
        "id": "tRUTPlFK9SJ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kC0BWwK99U8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 5: Python Program – Load CSV, Train Logistic Regression Model, and Print Accuracy\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = (data.target != 0).astype(int)\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,\n",
        "random_state=42)\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "# Evaluate model\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3O86-5h9abs",
        "outputId": "17e993a9-1701-4f3f-b575-79a0626cf402"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Question 6: Train Logistic Regression Model using L2 Regularization (Ridge)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target,\n",
        "test_size=0.3, random_state=42)\n",
        "# L2 Regularization\n",
        "model = LogisticRegression(penalty='l2', solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "# Evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Model Coefficients:\", model.coef_)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5k_bEkwT-165",
        "outputId": "55f6170a-0c6c-4c91-ef55-35fcf264901e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Coefficients: [[ 2.17532856e+00  1.59657795e-01 -1.25372350e-01 -4.00203956e-03\n",
            "  -1.30412639e-01 -4.11271449e-01 -6.55025779e-01 -3.50105949e-01\n",
            "  -2.02221998e-01 -2.92893734e-02 -6.61181920e-02  1.40364311e+00\n",
            "   1.17866280e-01 -1.09265346e-01 -1.46461555e-02 -2.48382696e-02\n",
            "  -6.34867207e-02 -4.11476085e-02 -4.87826550e-02 -7.69418228e-04\n",
            "   1.15519347e+00 -3.90327993e-01 -7.67924369e-02 -2.13242325e-02\n",
            "  -2.42143589e-01 -1.13976004e+00 -1.57934527e+00 -6.17350727e-01\n",
            "  -7.29143464e-01 -1.10785408e-01]]\n",
            "Accuracy: 0.9649122807017544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Question 7: Train Logistic Regression for Multiclass Classification using ‘ovr’\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target,\n",
        "test_size=0.3, random_state=42)\n",
        "# Train model\n",
        "model = LogisticRegression(multi_class='ovr', solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "# Evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYEkBAI7-67m",
        "outputId": "5eb7cc04-0a4f-422d-ec39-3b00ff04edd0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        19\n",
            "           1       1.00      0.92      0.96        13\n",
            "           2       0.93      1.00      0.96        13\n",
            "\n",
            "    accuracy                           0.98        45\n",
            "   macro avg       0.98      0.97      0.97        45\n",
            "weighted avg       0.98      0.98      0.98        45\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Question 8: Hyperparameter Tuning using GridSearchCV\n",
        "\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "# Load data\n",
        "data = load_breast_cancer()\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target,\n",
        "test_size=0.3, random_state=42)\n",
        "# Parameter grid\n",
        "param_grid = {'C': [0.01, 0.1, 1, 10], 'penalty': ['l1', 'l2']}\n",
        "# GridSearchCV\n",
        "grid = GridSearchCV(LogisticRegression(solver='liblinear'), param_grid, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best Validation Accuracy:\", grid.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qxbg5Tnv_CYx",
        "outputId": "aac06b81-cda8-4cf6-c3e0-970d033fd6ac"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 10, 'penalty': 'l1'}\n",
            "Best Validation Accuracy: 0.9697784810126582\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Question 9: Compare Accuracy with and without Standardization\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target,\n",
        "test_size=0.3, random_state=42)\n",
        "# Without scaling\n",
        "model1 = LogisticRegression(max_iter=1000)\n",
        "model1.fit(X_train, y_train)\n",
        "acc1 = accuracy_score(y_test, model1.predict(X_test))\n",
        "# With scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "model2 = LogisticRegression(max_iter=1000)\n",
        "model2.fit(X_train_scaled, y_train)\n",
        "acc2 = accuracy_score(y_test, model2.predict(X_test_scaled))\n",
        "print(\"Accuracy without scaling:\", acc1)\n",
        "print(\"Accuracy with scaling:\", acc2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjOsP5glKK1i",
        "outputId": "e5fc84e9-69b6-4220-80c0-21c546e28739"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.9707602339181286\n",
            "Accuracy with scaling: 0.9824561403508771\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Question 10: Real-World Business Case – Predicting Marketing Campaign Response\n",
        "\n",
        "\n",
        " Answer: In an e-commerce company, we want to predict which customers will respond to a marketing\n",
        " campaign where only 5% respond. The dataset is highly imbalanced, so a careful approach is required. First,\n",
        " the data should be cleaned and preprocessed, missing values handled, and categorical variables encoded.\n",
        " The dataset is then split into training and testing sets. To handle class imbalance, techniques such as\n",
        " SMOTE (Synthetic Minority Oversampling Technique) or class_weight='balanced' in logistic regression\n",
        " should be used.\n",
        " Next, numerical features should be standardized using StandardScaler to ensure uniformity. Logistic\n",
        " Regression with L2 regularization is used to avoid overfitting. Hyperparameter tuning can be done using\n",
        " GridSearchCV to find the optimal C and penalty values. The model should be evaluated using Precision,\n",
        " Recall, F1-score, and ROC-AUC instead of accuracy, due to the imbalanced nature of the dataset. Finally, an\n",
        " optimal probability threshold can be chosen to maximize business benefits. Following this approach\n",
        " ensures a robust and effective prediction model for marketing campaign responses."
      ],
      "metadata": {
        "id": "SzFqSCFQMfqK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C-WdPnJ-KU5u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}